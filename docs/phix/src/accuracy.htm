<head>
 <body>
  <toc>
   <these>
    <get>
     <stripped>
      <h1 class="title">Floats Are Not Exact</h1>
      <div id="mainSection">
<!--    Note that there are infinitely many real numbers, but only a finite number of them (18437736874454810627, 
        to be exact) can be represented exactly by the JavaScript floating-point format. This means that when 
        you&rsquo;re working with real numbers in JavaScript, the representation of the number will often be an 
        approximation of the actual number. The approximation is usually good enough, however, and this is rarely a practical problem. -->
        While all integer values (up to a certain limit, around 10<sup><small>15</small></sup> on 32-bit 
        and 10<sup><small>19</small></sup> on 64-bit) are always exact, the same cannot be said for 
        floating point numbers.
        <br>
        <br>
        As mentioned in Number Bases, computers hold numbers in binary, or base 2. And just as our familiar
        decimal system <b>cannot</b> hold <small><sup>1</sup>/<sub>3</sub></small> (one third) exactly, 
        neither can base 2, which also <b>cannot</b> hold <small><sup>1</sup>/<sub>5</sub></small> (one fifth) 
        exactly. It may only be out by the tiniest of amounts, less than, say, the width of an atom (pun
        intended), but it simply <b>cannot</b> be exact. The principle of "close enough" applies.
        <br>
        <br>
        0.1 in decimal is held in binary as 0.0001100110011001100110011001100110011001100110011... i.e.
        <small><sup>0</sup>/<sub>2</sub></small>+<small><sup>0</sup>/<sub>4</sub></small>+
        <small><sup>0</sup>/<sub>8</sub></small>+<small><sup>1</sup>/<sub>16</sub></small>+
        <small><sup>1</sup>/<sub>32</sub></small>+<small><sup>0</sup>/<sub>64</sub></small>+
        <small><sup>0</sup>/<sub>128</sub></small>+<small><sup>1</sup>/<sub>256</sub></small>+
        <small><sup>1</sup>/<sub>512</sub></small>+<small><sup>0</sup>/<sub>1024</sub></small>, etc.
        It gets pretty close, but would never manage to be exact, no matter how many digits you used.
        In fact the only fractions that can be held exactly are a half, quarter, eighth, etc, and any
        value that can be composed as an exact sum of such fractions.
        <br>
        <br>
        A related but different concept is precision: starting with a value of 1e300, you can try to add
        1 (or for that matter 1e250) to it as many times as you like; it will never make the slightest
        dent, not even if you leave it running for millenia. (Just something you should know.) Obviously
        if you multiply the 1 (or the 1e250) by something before the addition, that&rsquo;s different.
        The actual precision available is about 15 decimal digits on 32 bit and about 19 decimal digits 
        on 64 bit, and accordingly if the ratio between two numbers is too extreme, we find ourselves in
        one of those too small to make a dent situations. If we were only keeping numbers to two decimal
        places, then the most accurate answer for 3.00 plus 0.001 is still 3.00, and something similar 
        happens when we are limited to 15 or 19 significant decimal digits. You get a very different
        answer from ((15+(1e-30))-15) [ie 0] to ((15-15)+(1e-30)) [ie 1e-30], and likewise 1e300+1-1e300
        yields 0 whereas 1e300-1e300+1 yields 1.
        <br>
        <a name=bookshelf></a>
        <br>
        I also rather like the following analogy. You can use atoms to store integer values up to 
        9,007,199,254,740,992, but like a bookshelf without any end stops, if you need an extra bit 
        on the left, then one must fall off the right. In fact, between 9,007,199,254,740,992 and 
        18,014,398,509,481,984 you can only store even numbers, and between 18,014,398,509,481,984 and 
        36,028,797,018,963,968, you can only store numbers divisible by 4, and so on, until by the time 
        you get to 1e300 the "smallest increment" is around 1e285.
        <small>(The numbers just given all correspond to a 32-bit system; calculating the equivalents for 
        64-bit systems is left as an exercise for readers with way too much time on their hands.)</small>
        <br>
        <br>
        Of course such approximations occur in our familiar decimal system, and in the real world are more 
        prevalent than not. You and I might say 31mm, an engineer 31.147mm, and a physicist 31.147436892mm,
        before adding something completely incomprehensible about lengths being composed of an infinite series 
        of quantum probability functions being distorted by gravitational waves, or something like that. It
        can be annoying that you need to take extra care when dealing with pennies and cents, but it is 
        inevitable for any hardware that can also cope with the kind of numbers needed for astrophysics.
        <br>
        <br>
        These features of the physical hardware are common to all programming languages, at least when using 
        the native machine floating point hardware as opposed to a specialised fraction or decimal library 
        component (eg <a href="bigatom.htm">bigatom</a>), which would of course be considerably slower.
        <br>
        <br>
        In particular, <b><i>comparing floating point numbers for equality is almost always the wrong thing to do</i></b>.
        <br>
        <br>
        Instead of <code>a=10.0</code> use <code>a&gt9.99 and a&lt10.01</code>, and instead of 
        <code>a=b</code> use <code>abs(a-b)&lt;1e-6</code>, or similar. 
        <br>
        <br>
        Alternatively it might be reasonable to design an accounting system that internally works in whole pennies/cents, 
        and multiplies by 0.01 (or whatever) just before the display. Speaking of accounting systems, and more than wildly 
        exagerrating the danger, imagine that some odd 5p is actually being stored as 0.0497 - it would take a mere 17 
        additions, such that remainder(sum,0.05) is less than 0.0450, before a penny discrepancy suddenly materialised 
        out of nowhere. Of course in a real system, it would take more than a million million (10<sup><small>12</small></sup>)
        additions for such an error to arise, though that number tumbles once multiplication gets involved. 
        Perhaps converting fractions to whole pennies and back on a regular basis would help, the point being made here is 
        that there is always a way to minimise the impact of the tiny imperfections inherent in IEEE 754 floating points.
        <br>
        <br>
        It is for all these reasons that phix does not and never will permit floating point for loops; they simply do 
        not work as expected, besides it is trivial to use an integer control loop variable and a manually maintained 
        floating point value to achieve the desired results, without occasionally iterating once more or once less 
        than was originally intended. Of course the real problem with floating point for loops is the overwhelming
        temptation to embed equality comparisons in them, which as said above in bold italics, is usually wrong.
        <br>
        <br>
        See 
          <a id="ext437" style="color:#9B5565" 
            href="javascript:ExternalLink('ext437','http://en.wikipedia.org/wiki/Floating_point');">wikipedia
          </a>&nbsp;or 
          <a id="ext477" style="color:#9B5565" 
            href="javascript:ExternalLink('ext477','https://docs.python.org/2/tutorial/floatingpoint.html');">python docs
          </a>
         &nbsp;if for whatever reason you think I might be making all this stuff up.<img src="images/ksk-smile.png" />
        <br>
        <br>
        Note that floating point inaccuracies tend to be more evident on 64-bit. This is because phix exclusively uses 
        the standard FPU registers (st0..st7) which are fundamentally 80-bit; while 32-bit phix stores 64-bit floats,
        64-bit stores 80-bit floats. Imagine that instead of 64/80 binary bits we had 2/4 decimal places, and that some 
        calculation led to a result of 1.9994 - if we store to 4dp it stays 1.9994, whereas if we store to 2dp then (as
        if by magic) it becomes 2.0. There is a thin veneer of rounding modes that hide a multitude of sins in 32-bit,
        that are simply not available in 64-bit, at least not in the 1:1 way that most of the code has been translated.
        More specifically, on 32-bit we store 80-bit FPU registers in 64-bit floats, forcing a little bit of rounding,
        that simply does not happen when we store an 80-bit FPU register in an 80-bit float.
        These problems would quite probably evaporate if it[64-bit] used SSE/xmm0..7 [128-bit regs], which is probably 
        not all that technically difficult, but finding the time and motivation is a different matter.
        <br>
        <br>
      </div>
     </stripped>
    </get>
   </these>
  </toc>
 </body>
</head>
